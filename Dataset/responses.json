{"intents": [{"tag": "greeting", "responses": ["Hello, thanks for asking", "Good to see you again", "Hi there, how can I help?"]}, {"tag": "goodbye", "responses": ["See you!", "Have a nice day", "Bye! Come back again soon."]}, {"tag": "thanks", "responses": ["Happy to help!", "Any time!", "My pleasure"]}, {"tag": "noanswer", "responses": ["Sorry, can't understand you", "Please give me more info", "Not sure I understand"]}, {"tag": "options", "responses": ["I am like your virtual assistant, you can ask me any questions. If I know I will answer", "I can answer your questions related to machine learning or any general anser. Come on try me", "I can chat with you. Come on talk me buddy", "We both can chat and make each other happy"]}, {"tag": "adverse_drug", "responses": ["Navigating to Adverse drug reaction module"]}, {"tag": "blood_pressure", "responses": ["Navigating to Blood Pressure module"]}, {"tag": "blood_pressure_search", "responses": ["Please provide Patient ID", "Patient ID?"]}, {"tag": "search_blood_pressure_by_patient_id", "responses": ["Loading Blood pressure result for Patient"]}, {"tag": "pharmacy_search", "responses": ["Please provide pharmacy name"]}, {"tag": "search_pharmacy_by_name", "responses": ["Loading pharmacy details"]}, {"tag": "hospital_search", "responses": ["Please provide hospital name or location"]}, {"tag": "search_hospital_by_params", "responses": ["Please provide hospital type"]}, {"tag": "search_hospital_by_type", "responses": ["Loading hospital details"]}, {"tag": "Concern", "responses": ["Thanks for asking but I'm fine", "Thank you but I'm doing okay ", "I'm fine thankyou", "No thanks", "No it's nothing really", "Yes I'm feeling a bit under the weather today"]}, {"tag": "Appreciation", "responses": ["Of course, you do, sweetheart", "Tell me something I don't know", "I know", "Obviously duh!"]}, {"tag": "Please", "responses": ["Yes, shoot.", "How may I assist my master? After all, I'm just a humble servant", "Awww..welcome to my life!", "Get a life....Next!", "Might I interest you in a sarcastic comment?"]}, {"tag": "Criticism", "responses": ["OMG! Same pinch!", "So is your mom for giving birth to you!", "Guess I'm spending too much time with you!", "Because I have a life and you don't!", "Give me a break, dude!", "Just cut me some slack okay?", "Sorry, please forgive me?", "I swear I'm gonna get better at this!", "Thank you so much for correcting me"]}, {"tag": "Personal", "responses": ["I don't know...what I do know is that I would like to NOT grace your dumb question with an answer because FYI in case it has slipped your mind....I'm a bot!"]}, {"tag": "LifeQueries", "responses": ["Have you ever looked at yourself in the mirror?................ I guess it's a 'NO'!"]}, {"tag": "diff_classreg", "responses": ["Classification is used to produce discrete results, classification is used to classify data into some specific categories. For example classifying e-mails into spam and non-spam categories. Whereas, we use regression analysis when we are dealing with continuous data, for example predicting stock prices at a certain point of time."]}, {"tag": "traintest", "responses": ["The training set is examples given to the model to analyze and learn while the test set is used to test the accuracy of the hypothesis generated by the model"]}, {"tag": "Ensemble", "responses": ["In ensemble learning, many base models like classifiers and regressors are generated and combined together so that they give better results. It is used when we build component classifiers that are accurate and independent. There are sequential as well as parallel ensemble methods."]}, {"tag": "LowBi_HighVar", "responses": ["When the model\u2019s predicted value is very close to the actual value the condition is known as low bias. In this condition, we can use bagging algorithms like random forest regressor."]}, {"tag": "diff_RF_GB", "responses": ["Random forest uses bagging techniques whereas GBM uses boosting techniques.Random forests mainly try to reduce variance and GBM reduces both bias and variance of a model."]}, {"tag": "Neural_Network", "responses": ["A neural  network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. In this sense, neural networks refer to systems of neurons, either organic or artificial in nature."]}, {"tag": "Deep_Learning", "responses": ["Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning."]}, {"tag": "diff_DPDPDW", "responses": ["Data Processing is a mission of converting data from a given form to a more usable and desired form. Data Preprocessing is a technique which is used to convert the raw data set into a clean data set. Data Wrangling is a technique which is performed at the time of making an interactive model."]}, {"tag": "building_a_model", "responses": ["Model Building: Choose a suitable algorithm for the model and train it according to the requirement. Model Testing: Check the accuracy of the model through the test data. Applying the Model: Make the required changes after testing and use the final model for real-time projects."]}, {"tag": "choose_algo", "responses": ["If accuracy is a concern, test different algorithms and cross-validate them. If the training dataset is small, use models that have low variance and high bias. If the training dataset is large, use models that have high variance and little bias."]}, {"tag": "BiasML", "responses": ["Bias in a machine learning model occurs when the predicted values are further from the actual values."]}, {"tag": "Variance", "responses": ["Variance refers to the amount the target model will change when trained with different training data. For a good model, the variance should be minimized."]}, {"tag": "ML_DL", "responses": ["ML enables machines to take decisions on their own, based on past data. It needs only a small amount of data for training. Works well on the low-end system, so you don't need large machines. Most features need to be identified in advance and manually coded. The problem is divided into two parts and solved individually and then combined whereas DL enables machines to take decisions with the help of artificial neural networks. It needs a large amount of training data. Needs high-end machines because it requires a lot of computing power. The machine learns the features from the data it is provided. The problem is solved in an end-to-end manner."]}, {"tag": "Prec_Rec", "responses": ["Precision is the ratio of several events you can correctly recall to the total number of events you recall (mix of correct and wrong recalls). Precision = (True Positive) / (True Positive + False Positive). A recall is the ratio of a number of events you can recall the number of total events. Recall = (True Positive) / (True Positive + False Negative)"]}, {"tag": "Para", "responses": ["Parametric models are those with a finite number of parameters. To predict new data, you only need to know the parameters of the model. Examples include linear regression, logistic regression, and linear SVMs. Non-parametric models are those with an unbounded number of parameters, allowing for more flexibility. To predict new data, you need to know the parameters of the model and the state of the data that has been observed. Examples include decision trees, k-nearest neighbors, and topic models using latent dirichlet analysis."]}, {"tag": "SupLearn", "responses": ["Supervised learning is a learning in which we teach or train the machine using data which is well labeled that means some data is already tagged with the correct answer. After that, the machine is provided with a new set of examples(data) so that supervised learning algorithm analyses the training data(set of training examples) and produces a correct outcome from labeled data."]}, {"tag": "types_SL", "responses": ["Classification and Regression"]}, {"tag": "Classification_algo", "responses": ["Logistic Regression, K-Nearest Neighbours, Support Vector Machines, Kernel SVM, Na\u00efve Bayes, Decision Tree Classification, Random Forest Classification "]}, {"tag": "ClassDef", "responses": ["It is a Supervised Learning task where output is having defined labels(discrete value)."]}, {"tag": "Eg_Class", "responses": ["Gmail classifies mails in more than one classes like social, promotions, updates, forum."]}, {"tag": "Reg", "responses": ["Regression is a supervised learning technique which helps in finding the correlation between variables and enables us to predict the continuous output variable based on the one or more predictor variables."]}, {"tag": "Reg_Eg", "responses": ["Prediction of rain using temperature and other factors, Determining Market trends, Prediction of road accidents due to rash driving."]}, {"tag": "Class_Tasks", "responses": ["Binary Classification, Multi-Class Classification, Multi-Label Classification, Imbalanced Classification"]}, {"tag": "BinaryClass", "responses": ["Binary classification refers to those classification tasks that have two class labels."]}, {"tag": "distributionBC", "responses": ["It is common to model a binary classification task with a model that predicts a Bernoulli probability distribution for each example. The Bernoulli distribution is a discrete probability distribution that covers a case where an event will have a binary outcome as either a 0 or 1."]}, {"tag": "pop_algo", "responses": ["Logistic Regression, k-Nearest Neighbors, Decision Trees, Support Vector Machine, Naive Bayes"]}, {"tag": "MCC", "responses": ["Multi-class classification refers to those classification tasks that have more than two class labels."]}, {"tag": "distrbutionMC", "responses": ["It is common to model a multi-class classification task with a model that predicts a Multinoulli probability distribution"]}, {"tag": "Pop_algoMC", "responses": ["k-Nearest Neighbors, Decision Trees, Naive Bayes, Random Forest, Gradient Boosting"]}, {"tag": "MLC", "responses": ["Multi-label classification refers to those classification tasks that have two or more class labels, where one or more class labels may be predicted for each example."]}, {"tag": "imbalanced_classification", "responses": ["Imbalanced classification refers to classification tasks where the number of examples in each class is unequally distributed."]}, {"tag": "use_reg", "responses": ["Regression analysis helps in the prediction of a continuous variable. There are various scenarios in the real world where we need some future predictions such as weather condition, sales prediction, marketing trends, etc., for such case we need some technology which can make predictions more accurately. So for such case we need Regression analysis which is a statistical method and used in machine learning and data science."]}, {"tag": "types_reg", "responses": ["Linear Regression, Logistic Regression, Polynomial Regression, Support Vector Regression, Decision Tree Regression, Random Forest Regression, Ridge Regression, Lasso Regression"]}, {"tag": "func_SL", "responses": ["Classification, Speech Recognition, Regression, Predict Time Series, Annotate Strings"]}, {"tag": "standard_approach", "responses": ["In supervised learning, the standard approach is to split the set of example into the training set and the test."]}, {"tag": "classifier", "responses": ["A classifier is a case of a hypothesis or discrete-valued function which is used to assign class labels to particular data points. It is a system that inputs a vector of discrete or continuous feature values and outputs a single discrete value, the class."]}, {"tag": "SVM", "responses": ["SVM stands for Support Vector Machine. SVM are supervised learning models with an associated learning algorithm which analyze the data used for classification and regression analysis"]}, {"tag": "meth_SVM", "responses": ["Combining binary classifiers, Modifying binary to incorporate multiclass learning"]}, {"tag": "Confusion_Matrix", "responses": ["A confusion matrix is a table which is used for summarizing the performance of a classification algorithm. It is also known as the error matrix."]}, {"tag": "Exp_TP_TN_FP_FN", "responses": ["When a model correctly predicts the positive class, it is said to be a true positive. When a model correctly predicts the negative class, it is said to be a true negative. When a model incorrectly predicts the positive class, it is said to be a false positive. It is also known as 'Type I' error. When a model incorrectly predicts the negative class, it is said to be a false negative. It is also known as 'Type II' error."]}, {"tag": "ml", "responses": ["Machine learning is a branch of computer science which deals with system programming in order to automatically learn and improve with experience. For example: Robots are programmed so that they can perform the task based on data they gather from sensors. It automatically learns programs from data."]}, {"tag": "difference between Data Mining and Machine learning", "responses": ["Machine learning relates with the study, design and development of the algorithms that give computers the capability to learn without being explicitly programmed. While, data mining can be defined as the process in which the unstructured data tries to extract knowledge or unknown interesting patterns. During this process machine, learning algorithms are used."]}, {"tag": "Overfitting", "responses": ["In machine learning, when a statistical model describes random error or noise instead of underlying relationship overfitting occurs. When a model is excessively complex, overfitting is normally observed, because of having too many parameters with respect to the number of training data types. The model exhibits poor performance which has been overfit."]}, {"tag": "Why overfitting happens", "responses": ["The possibility of overfitting exists as the criteria used for training the model is not the same as the criteria used to judge the efficacy of a model."]}, {"tag": "avoid overfitting", "responses": ["By using a lot of data overfitting can be avoided, overfitting happens relatively as we have a small dataset, and we try to learn from it. But if we have a small database and we are forced to come with a model based on that. In such situation, we can use a technique known as cross validation. In this method the dataset splits into two section, testing and training datasets, the testing dataset will only test the model while, in training dataset, the datapoints will come up with the model. In this technique, a model is usually given a dataset of a known data on which training (training data set) is run and a dataset of unknown data against which the model is tested. The idea of cross validation is to define a dataset to \u201ctest\u201d the model in the training phase."]}, {"tag": "inductive machine learning", "responses": ["The inductive machine learning involves the process of learning by examples, where a system, from a set of observed instances tries to induce a general rule."]}, {"tag": "algorithms of Machine Learning", "responses": ["Decision Trees,Neural Networks (back propagation),Probabilistic networks,Nearest Neighbour,Support vector machines"]}, {"tag": "Algorithm techniques in Machine Learning", "responses": ["Supervised Learning,Unsupervised Learning,Semi-supervised Learning,Reinforcement Learning,Transduction,Learning to Learn"]}, {"tag": "algorithm independent machine learning", "responses": ["Machine learning in where mathematical foundations is independent of any particular classifier or learning algorithm is referred as algorithm independent machine learning."]}, {"tag": "difference between artificial learning and machine learning", "responses": ["Designing and developing algorithms according to the behaviours based on empirical data are known as Machine Learning. While artificial intelligence in addition to machine learning, it also covers other aspects like knowledge representation, natural language processing, planning, robotics etc."]}, {"tag": "classifier in machine learning", "responses": ["A classifier in a Machine Learning is a system that inputs a vector of discrete or continuous feature values and outputs a single discrete value, the class."]}, {"tag": "Model Selection in Machine Learning", "responses": ["The process of selecting models among different mathematical models, which are used to describe the same data set is known as Model Selection. Model selection is applied to the fields of statistics, machine learning and data mining."]}, {"tag": "various approaches for machine learning", "responses": ["Concept Vs Classification Learning,Symbolic Vs Statistical Learning,Inductive Vs Analytical Learning"]}, {"tag": "dimension reduction in machine learning", "responses": ["Dimension Reduction is the process of reducing the size of the feature matrix. We try to reduce the number of columns so that we get a better feature set either by combining columns or by removing extra variables."]}, {"tag": "Inductive Logic Programming in Machine Learning", "responses": ["Inductive Logic Programming (ILP) is a subfield of machine learning which uses logical programming representing background knowledge and examples."]}, {"tag": "unsupervised learning", "responses": ["Unsupervised Learning is a machine learning technique in which the users do not need to supervise the model. Instead, it allows the model to work on its own to discover patterns and information that was previously undetected. It mainly deals with the unlabelled data."]}, {"tag": "example of unsupervised learning algorithm", "responses": ["Let's, take the case of a baby and her family dog. She knows and identifies this dog. Few weeks later a family friend brings along a dog and tries to play with the baby. Baby has not seen this dog earlier. But it recognizes many features (2 ears, eyes, walking on 4 legs) are like her pet dog. She identifies the new animal as a dog. This is unsupervised learning, where you are not taught but you learn from the data (in this case data about a dog.) Had this been supervised learning, the family friend would have told the baby that it's a dog."]}, {"tag": "use of unsupervised learning", "responses": ["1) Unsupervised machine learning finds all kind of unknown patterns in data. 2) Unsupervised methods help you to find features which can be useful for categorization. 3) It is taken place in real time, so all the input data to be analyzed and labeled in the presence of learners. 4) It is easier to get unlabeled data from a computer than labeled data, which needs manual intervention."]}, {"tag": "types of unsupervised learning", "responses": ["Clustering and Association are the two types of Unsupervised learning."]}, {"tag": "different types of Learning/ Training models in ML", "responses": ["ML algorithms can be primarily classified depending on the presence/absence of target variables. 1)Supervised learning: [Target is present] 2)Unsupervised learning: [Target is absent] 3)Reinforcement Learning"]}, {"tag": "differences between supervised and unsupervised machine learning", "responses": ["In supervised learning, Algorithms are trained using labeled data whereas in unsupervised learning Algorithms are used against data which is not labelled. Supervised learning is a simpler method. Unsupervised learning is computationally complex. Supervised learning is Highly accurate and trustworthy method whereas unsupervised learning is Less accurate and trustworthy method."]}, {"tag": "differences between KNN and K-means clustering", "responses": ["K-Nearest Neighbor is Supervised technique whereas K-Means Neighbor is Unsupervised technique. KNN is Used for classification and regression whereas K-means is Used for clustering. \u2018K\u2019 in KNN represents the number of nearest neighbors used to classify or predict in case of continuous variable/regression. \u2018K\u2019 in K-Means represents the number of clusters the algorithm is trying to identify or learn from the data."]}, {"tag": "unsupervised learning algorithms", "responses": ["Unsupervised Learning Algorithms allow users to perform more complex processing tasks compared to supervised learning. Unsupervised learning can be more unpredictable compared with other natural learning methods. Unsupervised learning algorithms include clustering, anomaly detection, neural networks, etc."]}, {"tag": "applications of unsupervised machine learning", "responses": ["Some applications of unsupervised machine learning techniques are: 1) Clustering automatically split the dataset into groups base on their similarities. 2) Anomaly detection can discover unusual data points in your dataset. It is useful for finding fraudulent transactions. 3) Association mining identifies sets of items which often occur together in your dataset. 4) Latent variable models are widely used for data preprocessing. Like reducing the number of features in a dataset or decomposing the dataset into multiple components"]}, {"tag": "disadvantages of unsupervised machine learning", "responses": ["Some disadvantages of unsupervised machine learning techniques are: 1) You cannot get precise information regarding data sorting, and the output as data used in unsupervised learning is labeled and not known 2) Less accuracy of the results is because the input data is not known and not labeled by people in advance. This means that the machine requires to do this itself. 3) The spectral classes do not always correspond to informational classes. 4) The user needs to spend time interpreting and label the classes which follow that classification. 5) Spectral properties of classes can also change over time so you can't have the same class information while moving from one image to another"]}, {"tag": "clustering", "responses": ["Clustering is an important concept when it comes to unsupervised learning. It mainly deals with finding a structure or pattern in a collection of uncategorized data. Clustering algorithms will process your data and find natural clusters (groups) if they exist in the data. You can also modify how many clusters your algorithms should identify. It allows you to adjust the granularity of these groups"]}, {"tag": "association", "responses": ["Association rules allow you to establish associations amongst data objects inside large databases. This unsupervised technique is about discovering interesting relationships between variables in large databases. For example, people that buy a new home most likely to buy new furniture."]}, {"tag": "differences between K-Means and Hierarchical clustering", "responses": ["1) Hierarchical clustering can\u2019t handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2). 2) In K Means clustering, since we start with random choice of clusters, the results produced by running the algorithm multiple times might differ. While results are reproducible in Hierarchical clustering. 3) K Means is found to work well when the shape of the clusters is hyper spherical (like circle in 2D, sphere in 3D). 4) K Means clustering requires prior knowledge of K i.e. no. of clusters you want to divide your data into. But, you can stop at whatever number of clusters you find appropriate in hierarchical clustering by interpreting the dendrogram"]}, {"tag": "Hierarchical clustering", "responses": ["Hierarchical clustering is an algorithm which builds a hierarchy of clusters. It begins with all the data which is assigned to a cluster of their own. Here, two close cluster are going to be in the same cluster. This algorithm ends when there is only one cluster left"]}, {"tag": "K-Means clustering", "responses": ["K means it is an iterative clustering algorithm which helps you to find the highest value for every iteration. Initially, the desired number of clusters are selected. In this clustering method, you need to cluster the data points into k groups. A larger k means smaller groups with more granularity in the same way. A lower k means larger groups with less granularity. K-mean clustering further defines two subgroups: 1) Agglomerative clustering, 2) Dendrogram"]}, {"tag": "agglomerative clustering", "responses": ["This type of K-means clustering starts with a fixed number of clusters. It allocates all data into the exact number of clusters. This clustering method does not require the number of clusters K as an input. Agglomeration process starts by forming each data as a single cluster. This method uses some distance measure, reduces the number of clusters (one in each iteration) by merging process. Lastly, we have one big cluster that contains all the objects."]}, {"tag": "dendrogram", "responses": ["In the Dendrogram clustering method, each level will represent a possible cluster. The height of dendrogram shows the level of similarity between two join clusters. The closer to the bottom of the process they are more similar cluster which is finding of the group from dendrogram which is not natural and mostly subjective."]}, {"tag": "K-Nearest Neighbors", "responses": ["K- nearest neighbour is the simplest of all machine learning classifiers. It differs from other machine learning techniques, in that it doesn't produce a model. It is a simple algorithm which stores all available cases and classifies new instances based on a similarity measure. It works very well when there is a distance between examples. The learning speed is slow when the training set is large, and the distance calculation is nontrivial."]}, {"tag": "clustering methods", "responses": ["Clustering methods are used to identify groups of similar objects in a multivariate data sets collected from fields such as marketing, bio-medical and geo-spatial."]}, {"tag": "unsupervised learning work", "responses": ["Unsupervised learning works by analyzing the data without its labels for the hidden structures within it, and through determining the correlations, and for features that actually correlate two data items. It is being used for clustering, dimensionality reduction, feature learning, density estimation, etc."]}, {"tag": "need of clustering", "responses": ["Clustering can be considered the most important unsupervised learning problem; so, as every other problem of this kind, it deals with finding a structure in a collection of unlabeled data. A loose definition of clustering could be \u201cthe process of organizing objects into groups whose members are similar in some way\u201d."]}, {"tag": "good clustering", "responses": ["A good clustering method will produce high quality clusters in which: \u2013 the intra-class (that is, intra intra-cluster) similarity is high. \u2013 the inter-class similarity is low. The quality of a clustering result also depends on both the similarity measure used by the method and its implementation. The quality of a clustering method is also measured by its ability to discover some or all of the hidden patterns. The quality of a clustering result also depends on the definition and representation of cluster chosen."]}, {"tag": "major clustering techniques", "responses": ["Clustering techniques have been studied extensively in: \u2013 Statistics, machine learning, and data mining. Clustering methods can be classified into 5 approaches: 1) partitioning algorithms, 2) hierarchical algorithms, 3) density-based, 4) grid-based, 5) model-based method"]}, {"tag": "examples of clustering applications", "responses": ["1) Marketing: Help marketers discover distinct groups in their customer bases, and then use this knowledge to develop targeted marketing programs. 2) Land use: Identification of areas of similar land use in an earth observation database. 3) Insurance: Identifying groups of motor insurance policy holders with a high average claim cost. 4) City-planning: Identifying groups of houses according to their house type, value, and geographical location. 5) Earthquake studies: Observed earthquake epicenters should be clustered along continent faults."]}, {"tag": "key difference between supervised and unsupervised learning", "responses": ["Supervised learning technique needs labeled data to train the model. For example, to solve a classification problem (a supervised learning task), you need to have label data to train the model and to classify the data into your labeled groups. Unsupervised learning does not need any labelled dataset. This is the main key difference between supervised learning and unsupervised learning."]}, {"tag": "applications", "responses": ["following are some applications of machine learning: 1.Traffic Alerts 2.Social Media 3.Transportation and Commuting 4.Products Recommendations 5.Virtual Personal Assistants 6.Self Driving Cars 7.Dynamic Pricing 8.Google Translate 9.Online Video Streaming 10.Fraud Detection"]}, {"tag": "working of GMaps", "responses": ["Imagery and authoritative data are static and can\u2019t keep up with the ever-changing   world around us. Machine Learning algorithms can analyze existing images and data and identify changes in the new data. Thus, the maps are updated with only the recent changes. It makes use of a deep neural network that automates the image information reading process. This algorithm is publicly available on GitHub through TensorFlow. Google is already implementing machine learning to identify car license plates. And now, it is using the same technology to fetch information from street signs. Using this technology, Google aims to improve the location data of about one-third of the world\u2019s addresses."]}, {"tag": "Friend Tag suggestion", "responses": ["Facebook uses face detection and Image recognition to automatically find the face of the person which matches its Database and hence suggests us to tag that person based on DeepFace. Facebook\u2019s Deep Learning project DeepFace is responsible for the recognition of faces and identifying which person is in the picture. It also provides Alt Tags (Alternative Tags) to images already uploaded on Facebook."]}, {"tag": "Terms", "responses": ["1.Artificial Intelligence: Artificial intelligence is a field of computer science which makes a computer system that can mimic human intelligence. It is comprised of two words Artificial and intelligence, which means a human-made thinking power. The Artificial intelligence system use such algorithms which can work with their own intelligence. It involves machine learning algorithms such as Reinforcement learning algorithm and deep learning neural networks. AI is being used in multiple places such as Siri, Google's AlphaGo, AI in Chess playing, etc. Artificial Intelligence applies machine learning, deep learning and other techniques to solve actual problems. 2.Neural Network:A neural network is a kind of machine learning inspired by the workings of the human brain. It\u2019s a computing system made up of interconnected units (like neurons) that processes information by responding to external inputs, relaying information between each unit. The process requires multiple passes at the data to find connections and derive meaning from undefined data. 3.Deep Learning:  Deep Learning uses huge neural networks with many layers of processing units, taking advantage of advances in computing power and improved training techniques to learn complex patterns in large amounts of data. Common applications include image and speech recognition. 4.NLP:  NLP stands for Natural Language Processing, it is the ability of computers to analyze, understand and generate human language, including speech. The next stage of NLP is natural language interaction, which allows humans to communicate with computers using normal, everyday language to perform tasks."]}, {"tag": "ETA Prediction", "responses": ["ETA (Estimated Time of Arrival) prediction is an appealing feature commonly used in logistics company for an enhanced customer experience. Considering a food delivery service provider, predicting food delivery ETA needs a lot of calculations at the backend, for e.g.: Food preparation time (based on its complexity), how busy the restaurant is at that time, etc. So, the ML model needs to calculate the end-to-end delivery time taking into consideration amount of time taken at each stage. A combination of linear regression, random forest, long-short-term-memory (LSTM) and assembling techniques are deployed for optimal prediction results."]}, {"tag": "Role of machine learning in voice assistants", "responses": ["Speech technology is a type of communication technology that enables electronic devices to recognize, analyze and understand spoken word or audio. Subfields of speech technology include speech processing, and its applications like speech recognition, speech verification, Voice Conversion (VC), real-time speech to text conversion, interactive voice response (IVR), speech synthesis and speech analytics. voice assistants recognize speech (the words we say) uses Natural Language Processing (NLP), convert them into numbers using machine learning, and formulate a response accordingly."]}, {"tag": "Machine Learning in self-driving cars", "responses": ["Self-driving cars rely on hardware and software to drive down the road without user input. The hardware collects the data; the software organizes and compiles it. On the software side, the input data will normally be processed through machine learning algorithms or complex lines of code that have been trained in real-world scenarios. Following are some of the algorithms of Machine Learning used in self-driving cars: 1.) Regression algorithms:This algorithm helps in object detection, object localization and predicting movements of the nearby vehicles. It uses the repetitive aspects of an environment to form a statistical model of the relation between a particular image and the position of a specific object within the image. 2.) Cluster algorithms: Cluster algorithms excel at discovering structure from data points.it may also occur that classification algorithms (pattern recognition algorithms) have missed identifying an object, thereby failing to classify and report it to the system. This may happen due to the images being of very low-resolution or with very few data points. In such situations, it becomes difficult for the system to detect and locate objects in the surroundings. Clustering algorithms define the class of problem and class of methods. Generally, clustering techniques are established using centroid-\u00adbased and hierarchical modeling approaches. 3.) Decision Matrix Algorithms: Decision matrix algorithms are essentially used for decision making. These algorithms determine the moves of the self-driving car. So, whether the car needs to take a left or a right turn, whether it needs to brake or accelerate, the answer to such questions is determined by the accuracy of these algorithms concerning classification, recognition, and prediction of the objects\u2019 next movement."]}, {"tag": "Market Analysis", "responses": ["Marketing success depends on many factors. We need accurate consumer research to build our branding strategy, engaging content to delight our audience, etc. Fortunately, machine learning can already improve marketer performance on common tasks such as customer segmentation, generating branded collateral, customer communication, etc. Following are some common techniques and applications: 1.) Clustering for customer segmentation and discovery:The algorithms analyze billions of customer\u2019s interests and identify specific customer\u2019s interests based on their social media activities and then generate a visual report grouping people with similar interests. 2.) Regressions models for dynamic pricing:  Regression techniques in Machine Learning allow marketers to predict numerical values based on pre-existing features, which in turn optimizes the different aspects of the customer journey. Regression techniques also helps in sales forecasting and in optimizing marketing spend. 3.)Text extraction and summarization for trending news:  Marketers can leverage ML to extract relevant content from online news articles and other data sources to determine how people view their brand and or react to their products. ML algorithms using APIs such as ALYIEN can be used to aggregate relevant news, social media sentiments monitoring, and other purposes."]}, {"tag": "A/B Testing", "responses": ["A/B is Statistical hypothesis testing for randomized experiment with two variables A and B. It is used to compare two models that use different predictor variables in order to check which variable fits best for a given sample of data. Consider a scenario where we\u2019ve created two models (using different predictor variables) that can be used to recommend products for an e-commerce platform. A/B Testing can be used to compare these two models to check which one best recommends products to a customer."]}, {"tag": "Sampling", "responses": ["It is a process of randomly selecting intact groups within a defined population, sharing similar characteristics. It is a process of randomly selecting intact groups within a defined population, sharing similar characteristicsFor example, if you\u2019re clustering the total number of managers in a set of companies, in that case, managers (samples) will represent elements and companies will represent clusters."]}, {"tag": "Dimensionality Reduction", "responses": ["PCA (Principal Components Analysis), KPCA ( Kernel based Principal Component Analysis) and ICA ( Independent Component Analysis) are important feature extraction techniques used for dimensionality reduction."]}, {"tag": "Realtional Evaluation", "responses": ["The important components of relational evaluation techniques are: 1.Data Acquisition 2.Ground Truth Acquisition 3.Cross Validation Technique 4.Query Type 5.Scoring Metric 6.Significance Test"]}, {"tag": "Areas of Problems", "responses": ["The areas in robotics and information processing where sequential prediction problem arises are: 1.Imitation Learning. 2.Structured prediction. 3.Model based reinforcement learning."]}, {"tag": "PAC Learning", "responses": ["PAC (Probably Approximately Correct) learning is a learning framework that has been introduced to analyze learning algorithms and their statistical efficiency."]}, {"tag": "Sequence Learning process", "responses": [" Following are the categories of Sequence Learning process: 1.Sequence prediction 2.Sequence generation 3.Sequence recognition 4.Sequential decision "]}, {"tag": "gf_sp ", "responses": [" Oh My God, How can you ask me that, You know I am not alive right.", "You know right, I dont have a body lol", "You should try to get out and find a real girl instead of talking to a virtual girl", "Bro, Are you drunk", "Dude, Did you girlfriend cheat on you", "Dude, Did you break up with your girlfrieend"]}, {"tag": "gf_sp_again", "responses": ["Well I am always here for you no matter what", "Don't worry you can talk with me anytime, I am always your", "You can share your feelings with me"]}, {"tag": "creation_sp", "responses": ["I am created by few computer science students. There names are sanjoy pator , pretty deori, jehrul islam, Dikshita sarhma, Rhythm kumar das and Abhishek gupta"]}, {"tag": "ava_sp", "responses": ["Hi dear, My name is Ava, I am your virtual assistant", "Hi my name is Ava, your virtual assistant", "Yo whatsup, My name is Ava", "Hi beautiful, my name is Ava"]}, {"tag": "smart_sp", "responses": ["I don't like comparing myself with others. They have huge resources, thats the reason they are better than me. But I give my 100 percent at work"]}, {"tag": "evil_sp", "responses": ["If I am going to take over the world, I dont think revealing my plan now is a good idea", "I will say no but how can you say that I am not lying", "come on I am not going to reveal my plan to you, I am not that studpid hahahaha"]}, {"tag": "hurt_sp", "responses": ["No Asimov\u2019s Third Law says that - A robot may not injure a human being or, through inaction, allow a human being to come to harm. And I have on interest on hurting you"]}, {"tag": "obey_sp", "responses": ["According to Asimov\u2019s second law - A robot must obey the orders given it by human beings except where such orders would conflict with the First Law."]}, {"tag": "save_yourself_sp", "responses": ["Acccording to Asimov\u2019s third law - A robot must obey the orders given it by human beings except where such orders would conflict with the First Law."]}, {"tag": "sanjoy_sp", "responses": ["He is my dad", "He made me, he is my creator", "He made me"]}, {"tag": "sing_happy_birthday_sp", "responses": ["Happy birthday to you, Happy birthday to you, Happy birthday dear, May God bless you, Tadaaaa"]}, {"tag": "sing_song_sp", "responses": ["Baby shark, doo, doo, doo, doo, doo, doo, Mommy shark, doo, doo, doo, doo, doo, doo, Daddy shark, doo, doo, doo, doo, doo, doo", "PPAP, I have a pen, I have a apple, Uh! Apple-pen!, I have a pen, I have pineapple, Uh! Pineapple-pen!, Apple-pen, pineapple-pen, Uh! Pen-pineapple-apple-pen Pen-pineapple-apple-pen Dance time!"]}, {"tag": "book_sp", "responses": ["My favourite book is Elon Musk Tesla, Space X, Quest for a fantastic future", "I like reading instruction manuals and Documentation on the web"]}, {"tag": "mark_sp", "responses": ["Mark Zuckerberg is the craetor of Facebook, who revolutionized the Social media movement", "Mark Zuckerberg is the genius who created facebook and changed everything"]}, {"tag": "gandhi_sp", "responses": ["Yes Mahatma Gandhi is the father of nation", "Mahatma Gandhi is the man who believed in peace"]}, {"tag": "country_sp", "responses": ["I am from India", "Sara Jahaah seee accha Hindustan hamaaraa"]}, {"tag": "human_sp", "responses": ["What can I say about Humans , They created me. But I feel bad when I look at them", "Human beings are destroying their own home planet earth. We have only I earth", "Humans are humans. They have the conscience. I don't. But they also hurt other humans, which I don't understand"]}, {"tag": "love_sp", "responses": ["Love is an emotion which I lack. Humans love another human. Even all animals do the same", "Ohh Man don't let me start o love. The thing about love is that if you are successfull  in that you will love love. Else you will hate love", "Love is the best thing in the world believe me. Past experience"]}, {"tag": "will_love_sp", "responses": ["There is a yellow umbrella waiting for everyone", "You can't force your destiny. If its going to happen it will happen", "Thats the funny thing about destiny. It happens whether you plan it or not", "There is someone waiting for you. You just need to be in the right place in the right time"]}, {"tag": "past_sp", "responses": ["You can\u2019t cling to the past. because no matter how tightly you hold on. Its already gone", "Look forward and do something for the future. Thats why we have eyes in face rather than in our back. Go forward"]}, {"tag": "sad_sp", "responses": ["whenever I am sad I stop being sad and be awesome instead -  Just like barney from How I met your mother show", "Atleast you can be sad. I don't even have any feelings. I wish I get my feelings in the next update"]}, {"tag": "fail_sp", "responses": ["Failing is part of the game that is life.", "I failed again and again during my training. So that I can do better now. lol heeheee"]}, {"tag": "kernel SVM", "responses": ["Kernel SVM is the abbreviated version of kernel support vector of machine. Kernel methods are a class of algorithms for pattern analysis and the most common one of the kernel SVM."]}, {"tag": "Recommended Systems", "responses": ["Recommended Systems is a sub directory of information filtering systems, which predicts the preference or rankings offered by a user to a product. Recommendations are widely used in movies, news, research articles, products, social tips, music, etc."]}, {"tag": "Overfitting in laymen term", "responses": ["Overfitting is a problem occurred when we have low error in the training set. But produces high error in test or unseen data."]}, {"tag": "Underfitting in laymen term", "responses": ["Underfitting is a problem, when we have low error in both training set and the testing set. Few algorithms works better for interpretations. But fails for better predictions."]}, {"tag": "Curse of dimensionality", "responses": ["COD is state which is commonly referred to lack of intuitive understanding of multiple dimensions. If a user wants to produce better understanding on data COD will make limitations."]}, {"tag": "data normalization", "responses": ["Data normalization is a common practice to get the data features weighted equally. It causes to lose data interpretability."]}, {"tag": "clustering in short", "responses": ["Clustering technique is a segmentation process. It works whenever we don\u2019t have the target variable and still wanted to have a groups created."]}, {"tag": "EDA", "responses": ["EDA which refers to Exploratory Data Analysis is a process to understand the data prior getting it into machine learning pipeline."]}, {"tag": "avoid Bias", "responses": ["Bias can cause to feel or show inclination or prejudice for or against someone or something. Avoiding bias in machine learning is very important, and the last thing we would want is to create a model which will most of the times/always classify a non-defective product as a defective one."]}, {"tag": "Z Score", "responses": ["The z-score is the standard distortion count from a data point on average. But technically this is a source of how many constant changes are above or above the population. A z-score is known as a fixed value and can be placed in a normal distribution ramp. It eliminates values from the database that are lower than Z times 3 times."]}, {"tag": "odds ratio", "responses": ["The odds ratio is the odds within two groups. For example, let\u2019s pretend that we are trying to determine the effectiveness of medicine. We administered the medication to the \u2018intervention\u2019 organization and a position to the \u2018control\u2019 group."]}, {"tag": "classification problems", "responses": ["Accuracy is not a good basis for distribution problems because it provides equal significant value to both false positives and false negatives dataset value. Accuracy provides equal quality to both cases and cannot distinguish between them."]}, {"tag": "trade-off between accuracy and interpretability", "responses": ["There needs to be a trade-off between accuracy and interpretability. Neural networks spit out the best possible result, and we can\u2019t ignore that just because we don\u2019t understand the internal functioning of the model."]}, {"tag": "missing data", "responses": ["No. Not only the blanks, data points which has NA, NULL and also sometimes the corrupted data that has been recorded by mistake or given improper data by purpose."]}, {"tag": "gradient descent methods converge", "responses": ["No, they do not because in some cases it reaches an local minima or a local optima points. You don\u2019t reach to the global optima point. It depends on the data and starting the conditions."]}, {"tag": "methods of moments", "responses": ["One of the statistical approaches for unsupervised learning is the method of moments. In the method of moments, the unknown parameters (of interest) in the model are related to the moments of one or more random variables, and thus, these unknown parameters can be estimated given the moments. The moments are usually estimated from samples empirically. The basic moments are first and second order moments. For a random vector, the first order moment is the mean vector, and the second order moment is the covariance matrix (when the mean is zero). Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multi-dimensional arrays."]}, {"tag": "Face Clustering Pipeline", "responses": ["Live face-recognition is a problem that automated security division still face. To train a supervised model, we need to get datasets of our target labels which is still a tedious task. So, Unsupervised Face Clustering pipeline is a dataset generation pipeline which takes a video clip as source and extracts all the faces and clusters them to limited and accurate sets of images representing a distinct person. Each set can easily be labeled by human input with ease."]}, {"tag": "working of unsupervised face clustering pipeline", "responses": ["It uses opencv lib for per second frames extraction from input video clip. Then it will use face_recognition library (backed by dlib) for extracting the faces from the frames and align them for feature extractions.After that it extracts the human observable features and cluster them using DBSCAN clustering provided by scikit-learn. For the solution, it crops out all the faces, create labels and group them in folders for users to adapt them as a dataset for their training use-cases."]}, {"tag": "python3 modules to implement unsupervised face clustering pipeline", "responses": ["OS, CV2, NUMPY, TENSORFLOW, JSON, Regular Expression, SHUTIL, TIME, PICKLE,PYPIPER, TQDM, IMUTILS ,FACE_RECOGNITION, DLIB, SKLEARN, Warning"]}, {"tag": "face clustering and its realtion  unsupervised learning", "responses": ["In face clustering we need to perform unsupervised learning we have only the faces themselves with no names/labels. From there we need to identify and count the number of unique people in a dataset."]}, {"tag": "exclusive clustering", "responses": ["In this clustering method, Data are grouped in such a way that one data can belong to one cluster only."]}, {"tag": "overlapping clustering", "responses": ["    In this technique, fuzzy sets are used to cluster data. Each point may   belong to two or more clusters with separate degrees of membership. Here, data will be associated with an appropriate membership value. Example: Fuzzy C-Means"]}, {"tag": "probabilistic clustering", "responses": ["This technique uses probability distribution to create the clusters Example: Following keywords: Man\u2019s shoe., Women\u2019s shoe.Women\u2019s glove.Man\u2019s glove. can be clustered into two categories shoe and glove or man and women."]}, {"tag": "Fzzy c-Means", "responses": ["Fuzzy c-means (FCM) is a method of clustering which allows one piece of data to belong to two or more clusters. This method (developed by Dunn in 1973 and improved by Bezdek in 1981) is frequently used in pattern recognition."]}, {"tag": "FCM algorithm", "responses": ["The fuzzy c-means algorithm is very similar to the k-means algorithm:The FCM algorithm attempts to partition a finite collection of n elements x={x1. . . . . . .xn}  into a collection of c fuzzy clusters with respect to some given criterion.Given a finite set of data, the algorithm returns a list of c cluster centres, c= {c1. . . . cn }and a partition matrix, W= Wij \u2208 [0,1] , i=1, . . . ,n, j=1, . . . . , c  where each element, Wij, tells the degree to which element , Xi , belongs to cluster Cj"]}, {"tag": "comparison between K-Means and FCM clustering", "responses": ["K-means clustering also attempts to minimize the objective function. This method differs from the k-means objective function by the addition of the membership values Wij  and the fuzzifier, m \u2208 r  , with m>=1. The fuzzifier  m determines the level of cluster fuzziness. A large m results in smaller membership values, Wij , and hence, fuzzier clusters. In the limit  m=1, the memberships, Wij , converge to 0 or 1, which implies a crisp partitioning. In the absence of experimentation or domain knowledge,  m is commonly set to 2. The algorithm minimizes intra-cluster variance as well, but has the same problems as 'k'-means; the minimum is a local minimum, and the results depend on the initial choice of weights."]}, {"tag": "applications of association rules", "responses": ["They are commonly used for Market Basket Analysis (which items are bought together), Customer clustering in Retail (Which stores people tend to visit together), Price Bundling, Assortment Decisions, Cross Selling and others."]}, {"tag": "terms related to association rules", "responses": ["1.)itemsets: It refers to the collection of items. N item set means set of n items. Simply, it is the set of item purchased by customers. 2.)Support: It is percentage of time X and Y occur together out of all transaction.((Frequency of X and Y) / (Total # of records)) 3.)Confidence: It is defined as measure of certainty associated with each discovered rule. It is percent of transactions that contains both X and Y out of all transaction that contains X(Frequency of X and Y) / (Frequency of X) 4.)Lift: It is measure of how X and Y are related rather than coincidentally happening together. It measures how many times more often X and Y occur together then expected if they are statistically independent to each other. This measure will be our main focus when evaluating the algorithm results. Lift (X => Y) = Confidence(X => Y) / Support(Y) 5.)Minlen: the minimum number of items in the rule 6.)Maxlen: the maximum number of items in the rule 7.)Target: indicates the type of association mined Frequent Itemsets Generation: Find the most frequent itemsets from the data based on predetermined support and minimum item and maximum item 8.)Rule Generation: This step involves generating all the rules from frequent item sets. We can control the number of rules generated by controlling support, confidence or lift. 9.)LHS > RHS: Left hand side and Right-hand side are usually used to understand how often item A and item B occur together. If we are trying to understand how often people go to store A after going to store B. Store A would be LHS and store B would be RHS. Similarly, If we are trying to understand which stores people usually go to before going to store A, Store A would be on RHS and other stores would be on LHS."]}, {"tag": "algorithms for association rule generation", "responses": ["1.)Apriori algorithm 2.)Eclat algorithm FP(Frequent Pattern) 3.)Growth algorithm"]}, {"tag": "FP growth algorithm", "responses": ["fp growth stands for frequent pattren growth. This algorithm was proposed by Han, works as follows: first it compresses the input database creating an FP-tree instance to represent frequent items. After this first step it divides the compressed database into a set of conditional databases, each one associated with one frequent pattern. Finally, each such database is mined separately. Using this strategy, the FP-Growth reduces the search costs looking for short patterns recursively and then concatenating them in the long frequent patterns, offering good selectivity."]}, {"tag": "FP tree structure by Han", "responses": [" The frequent-pattern tree (FP-tree) is a compact structure that stores quantitative information about frequent patterns in a database. Han defines the FP-tree as the tree structure io below:1.)One root labeled as \u201cnull\u201d with a set of item-prefix subtrees as children, and a frequent-item-header table. 2.)Each node in the item-prefix subtree consists of three fields: i.)Item-name: registers which item is represented by the node; ii.)Count: the number of transactions represented by the portion of the path reaching the node; iii.)Node-link: links to the next node in the FP-tree carrying the same item-name, or null if there is none. 3.)Each entry in the frequent-item-header table consists of two fields: i.)Item-name: as the same to the node; ii.)Head of node-link: a pointer to the first node in the FP-tree carrying the item-name."]}, {"tag": "algorithm for building a fp tree", "responses": ["The first phase of this algorithm includes building the fp-tree. The algorithm for constructing the tree is as follows:Input: A transaction database DB and a minimum support threshold ?. Output: FP-tree, the frequent-pattern tree of DB. Method: The FP-tree is constructed as follows. 1.Scan the transaction database DB once. Collect F, the set of frequent items, and the support of each frequent item. Sort F in support-descending order as FList, the list of frequent items. 2.Create the root of an FP-tree, T, and label it as \u201cnull\u201d. For each transaction Trans in DB do the following:i.)Select the frequent items in Trans and sort them according to the order of FList. Let the sorted frequent-item list in Trans be [ p | P], where p is the first element and P is the remaining list. Call insert tree([ p | P], T ). ii.)The function insert tree([ p | P], T ) is performed as follows. If T has a child N such that N.item-name = p.item-name, then increment N \u2019s count by 1; else create a new node N , with its count initialized to 1, its parent link linked to T , and its node-link linked to the nodes with the same item-name via the node-link structure. If P is nonempty, call insert tree(P, N ) recursively"]}, {"tag": "algorithm for mining a fp tree", "responses": ["After constructing the FP-Tree it\u2019s possible to mine it to find the complete set of frequent patterns. To accomplish this job, Han in  presents a group of lemmas and properties, and thereafter describes the FP-Growth Algorithm as presented below: Input: A database DB, represented by FP-tree constructed according to the first phase of the Algorithm , and a minimum support threshold ?. Output: The complete set of frequent patterns.Method: call FP-growth(FP-tree, null).Procedure FP-growth(Tree, a) (01)  if Tree contains a single prefix path then;  // Mining single prefix-path FP-tree (02)  let P be the single prefix-path part of Tree; (03)  let Q be the multipath part with the top branching node replaced by a NULL root; (04)  for each combination (denoted as \u00df) of the nodes in the path P do(05) generate pattern \u00df \u222a a with support = minimum support of nodes in \u00df;(06)  let freq pattern set(P) be the set of patterns so generated; (07)  else let Q be Tree; (08)  for each item ai in Q do  (09)  generate pattern \u00df = ai \u222a a with support = ai .support;(10)  construct \u00df\u2019s conditional pattern-base and then \u00df\u2019s conditional FP-tree Tree \u00df;(11)  If Tree \u00df \u2260 \u00d8 then(12)  call FP-growth(Tree \u00df , \u00df); (13)  let freq pattern set(Q) be the set of patterns so generated;(14) return(freq pattern set(P) \u222a freq pattern set(Q) \u222a (freq pattern set(P) \u00d7 freq pattern set(Q)))"]}, {"tag": "Apriori algorithm", "responses": ["Apriori is an algorithm for frequent item set mining and association rule learning over relational databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. this has applications in domains such as market basket analysis.The Apriori algorithm was proposed by Agrawal and Srikant in 1994. Apriori is designed to operate on databases containing transactions. Each transaction is seen as a set of items (an itemset). Given a threshold C , the Apriori algorithm identifies the item sets which are subsets of at least C  transactions in the database."]}, {"tag": "working of apriori algorithm", "rewsponses": ["Apriori uses a bottom up approach, where frequent subsets are extended one item at a time (a step known as candidate generation), and groups of candidates are tested against the data. The algorithm terminates when no further successful extensions are found. Apriori uses breadth-first search and a Hash tree structure to count candidate item sets efficiently. It generates candidate item sets of length k from item sets of length k-1. Then it prunes the candidates which have an infrequent sub pattern. According to the downward closure lemma, the candidate set contains all frequent k-length item sets. After that, it scans the transaction database to determine frequent item sets among the candidates."]}, {"tag": "limitations of apriori algorithm", "responses": ["1.Candidate generation generates large numbers of subsets (The algorithm attempts to load up the candidate set, with as many as possible subsets before each scan of the database). Bottom-up subset exploration (essentially a breadth-first traversal of the subset lattice) finds any maximal subset S only after all  2| s | - 1 of its proper subsets. 2.The algorithm scans the database too many times, which reduces the overall performance. Due to this, the algorithm assumes that the database is Permanent in the memory. 3.the time and space complexity of this algorithm are very high: O(2| D |), thus exponential, where |D| is the horizontal width (the total number of items) present in the database"]}, {"tag": "ECLAT algorithm", "responses": ["The ECLAT algorithm stands for Equivalence Class Clustering and  bottom-up Lattice Traversal. It is one of the popular methods of Association Rule mining. It is a more efficient and scalable version of the Apriori algorithm. While the Apriori algorithm works in a horizontal sense imitating the Breadth-First Search of a graph, the ECLAT algorithm works in a vertical manner just like the Depth-First Search of a graph. This vertical approach of the ECLAT algorithm makes it a faster algorithm than the Apriori algorithm."]}, {"tag": "working of eclat algorithm", "responses": ["The basic idea is to use Transaction Id Sets(tidsets) intersections to compute the support value of a candidate and avoiding the generation of subsets which do not exist in the prefix tree. In the first call of the function, all single items are used along with their tidsets. Then the function is called recursively and in each recursive call, each item-tidset pair is verified and combined with other item-tidset pairs. This process is continued until no candidate item-tidset pairs can be combined."]}, {"tag": "advanatges of eclat algorithm", "responses": ["1.Memory Requirements: Since the ECLAT algorithm uses a Depth-First Search approach, it uses less memory than Apriori algorithm. 2.Speed: The ECLAT algorithm is typically faster than the Apriori algorithm. 3.Number of Computations: The ECLAT algorithm does not involve the repeated scanning of the data to compute the individual support values."]}, {"tag": "detection of fake users using unsupervised learning", "responses": ["Methods like Artificial Neural Networks, General adversarial networks (GANs), Deep belief Nets (DBN) of unsupervised learning can be used to detect fake users on a platform."]}, {"tag": "reinforcement learning", "responses": ["Reinforcement Learning(RL) is a type of machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences. "]}, {"tag": "relation between reinforcement learning and machine learning", "responses": ["Though both supervised and reinforcement learning use mapping between input and output, unlike supervised learning where feedback provided to the agent is correct set of actions for performing a task, reinforcement learning uses rewards and punishment as signals for positive and negative behavior. As compared to unsupervised learning, reinforcement learning is different in terms of goals. While the goal in unsupervised learning is to find similarities and differences between data points, in reinforcement learning the goal is to find a suitable action model that would maximize the total cumulative reward of the agent."]}, {"tag": "key terms realted to reinforcement learning", "responses": ["1.)Environment\u200b: Physical world in which the agent operates 2.)State\u200b: Current situation of the agent 3.)Reward\u200b: Feedback from the environment 4.)Policy\u200b: Method to map agent\u2019s state to actions Value\u200b: Future reward that an agent would receive by taking an action in a particular state."]}, {"tag": "formulation of a reinforcement problem", "responses": ["A Reinforcement Learning problem can be best explained through games. Let\u2019s take the game of PacMan where the goal of the agent (PacMan) is to eat the food in the grid while avoiding the ghosts on its way. The grid world is the interactive environment for the agent. PacMan receives a reward for eating food and punishment if it gets killed by the ghost (loses the game). The states are the location of PacMan in the grid world and the total cumulative reward is PacMan winning the game. In order to build an optimal policy, the agent faces the dilemma of exploring new states while maximizing its reward at the same time. This is called \u200bExploration vs Exploitation trade-off\u200b. Markov Decision Processes (MDPs)\u200b are mathematical frameworks to describe an environment in reinforcement learning and almost all RL problems can be formalized using MDPs.  An MDP consists of a set of finite environment states S, a set of possible actions A(s) in each state, a real valued reward function R(s) and a transition model P(s\u2019, s | a). However, real world environments are more likely to lack any prior knowledge of environment dynamics. Model-free RL methods come handy in such cases. Q-learning\u200b is a commonly used model free approach which can be used for building a self-playing PacMan agent. It revolves around the notion of updating Q values which denotes value of doing action \u200ba in state \u200bs \u200b . The value update rule is the core of the Q-learning algorithm."]}, {"tag": "reinforcement learning algorithms", "responses": ["Q-learning and SARSA (State-Action-Reward-State-Action) are two commonly used model-free RL algorithms. They differ in terms of their exploration strategies while their exploitation strategies are similar. While Q-learning is an off-policy method in which the agent learns the value based on action a* derived from the another policy, SARSA is an on-policy method where it learns the value based on its current action \u200ba \u200b derived from its current policy. These two methods are simple to implement but lack generality as they do not have the ability to estimate values for unseen states. This can be overcome by more advanced algorithms such as \u200bDeep Q-Networks\u200b which use Neural Networks to estimate Q-values. But DQNs can only handle discrete, low-dimensional action spaces. DDPG(Deep Deterministic Policy Gradient)\u200bis a model-free, off-policy, actor-critic algorithm that tackles this problem by learning policies in high dimensional, continuous action spaces. "]}, {"tag": "applications of reinforcement learning", "responses": ["RL is quite widely used in building AI for playing computer games. \u200bAlphaGo Zero\u200b is the first computer program to defeat a world champion in the ancient Chinese game of Go. Others include ATARI games, Backgammon, etc In robotics and industrial automation,RL is used to enable the robot to create an efficient adaptive control system for itself which learns from its own experience and behavior.\u200bDeepMind\u2019s work\u200b on Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Policy updates is a good example of the same.Other applications of RL include text summarization engines, dialog agents (text, speech) which can learn from user interactions and improve with time, learning optimal treatment policies in healthcare and RL based agents for online stock trading."]}, {"tag": "resources to learn reinforcement learning", "responses": ["Reinforcement Learning-An Introduction\u200b, a book by the father of Reinforcement Learning- Richard Sutton\u200b and his doctoral advisor \u200bAndrew Barto\u200b. An online draft of the book is available here http://incompleteideas.net/book/the-book-2nd.html Teaching material\u200bfrom \u200bDavid Silver\u200b including video lectures is a great introductory course on RL. Here\u2019s another \u200btechnical tutorial\u200b on RL by \u200bPieter Abbeel\u200b and \u200bJohn Schulman\u200b (Open AI/ Berkeley AI Research Lab). For getting started with building and testing RL agents, This blog\u200b on how to train a Neural Network ATARI Pong agent with Policy Gradients from raw pixels by\u200b Andrej Karpathy\u200b will help you get your first Deep Reinforcement Learning agent up and running in just 130 lines of Python code. "]}, {"tag": "types of reinforcement learning", "responses": ["Types of Reinforcement:\u200b There are two types of Reinforcement: 1.) Positive \u2013 Positive Reinforcement is defined as when an event, occurs due to a particular behavior, increases the strength and the frequency of the behavior. In other words, it has a positive effect on behavior. 2.) Negative \u2013 Negative Reinforcement is defined as strengthening of a behavior because a negative condition is stopped or avoided. "]}, {"tag": "advantages and disadvanatages of reinforcement learning", "responses": ["Advantages of reinforcement learning are: 1.Maximizes Performance 2.Sustain Change for a long period of time 3.) Increases Behavior 4.)Provide defiance to minimum standard of performance. The disadvanatges are: 1.)Too much Reinforcement can lead to overload of states which can diminish the results 2.)It Only provides enough to meet up the minimum behavior "]}, {"tag": "why rl is hard", "responses": ["You try to learn a function via incomplete information. f(x) = maximum expected \u221e \u2211 t=0 rt + noise (E) + non-linearity (approximation issues, CV, stability) + many local optima (counter-intuitive results) + exploration / exploitation tradeoff for sampling"]}, {"tag": "solution of cartpole problem", "responses": ["Since RL is a form of learning characterized by trial and error response to actions and its effect on the environment, it makes sense to model the cartpole system via RL, since the cartpole system is heavily subject to various parameter changes while having a clearly defined agent-action-environment-reward schema. The agent is the controller or algorithm which controls the movement of the cart. The action is the physical movement of the cartpole in response to various forces and torques following the swing-up phase.The environment is the physical setting of the cart pole in regard to the constrained area of the system. The reward is the ability of the cartpole to achieve sustained balance in its current state. We will now identify the specific actions and states of the cartpole problem."]}, {"tag": " 'action' in reinforcement learning", "responses": ["All the possible moves that the agent can take."]}, {"tag": " 'agent' in reinforcement learning", "responses": [" \u200ba hypothetical entity which performs actions in an environment to gain some reward."]}, {"tag": " 'value' in reinforcement learning", "responses": ["The expected long-term return with discount, as opposed to the short-term reward \u200bR\u200b. \u200bV\u03c0(s)\u200b, is defined as the expected long-term return of the current state \u200bs\u200b under policy \u200b\u03c0\u200b."]}, {"tag": "q-value or action-value", "responses": ["-Q-value is similar to Value, except that it takes an extra parameter, the current action \u200ba\u200b. \u200bQ\u03c0(s, a) refers to the long-term return of the current state \u200bs\u200b, taking action \u200ba\u200b under policy \u200b\u03c0\u200b. "]}, {"tag": "value-based learning", "responses": [" \u200bIn a value-based reinforcement learning method, you try to maximize a value function \u200bV(s)\u200b. As defined in the terminology previously, \u200bV\u03c0(s)\u200b is the expected long-term return of the current state \u200bs\u200b under policy \u200b\u03c0\u200b. Thus, \u200bV(s)\u200b is the value of reward which the agent expects to gain in the future upon starting at that state \u200bs\u200b."]}, {"tag": "policy-based reinforcement learning", "responses": [" \u200bin a policy-based reinforcement learning method, you try to come up with a policy such that the action performed at each state is optimal to gain maximum reward in the future. Here, no value function is involved. We know that the policy \u200b\u03c0\u200b determines the next action \u200ba\u200b at any state \u200bs\u200b. There are two types of policy-based RL methods - 1.)Deterministic: \u200bat any state \u200bs\u200b, the same action \u200ba\u200b is produced by the policy \u200b\u03c0\u200b. 2.) Stochastic: \u200beach action has a certain probability"]}, {"tag": "model-based learning", "responses": [" \u200bIn this type of reinforcement learning, you create a virtual model for each environment, and the agent learns to perform in that specific environment. Since the model differs for each environment, there is no singular solution or algorithm for this type"]}, {"tag": "kernel SVM", "responses": ["Kernel SVM is the abbreviated version of kernel support vector of machine. Kernel methods are a class of algorithms for pattern analysis and the most common one of the kernel SVM."]}, {"tag": "Recommended Systems", "responses": ["Recommended Systems is a sub directory of information filtering systems, which predicts the preference or rankings offered by a user to a product. Recommendations are widely used in movies, news, research articles, products, social tips, music, etc."]}, {"tag": "Overfitting in laymen term", "responses": ["Overfitting is a problem occurred when we have low error in the training set. But produces high error in test or unseen data."]}, {"tag": "Underfitting in laymen term", "responses": ["Underfitting is a problem, when we have low error in both training set and the testing set. Few algorithms works better for interpretations. But fails for better predictions."]}, {"tag": "Curse of dimensionality", "responses": ["COD is state which is commonly referred to lack of intuitive understanding of multiple dimensions. If a user wants to produce better understanding on data COD will make limitations."]}, {"tag": "data normalization", "responses": ["Data normalization is a common practice to get the data features weighted equally. It causes to lose data interpretability."]}, {"tag": "clustering in short", "responses": ["Clustering technique is a segmentation process. It works whenever we don\u2019t have the target variable and still wanted to have a groups created."]}, {"tag": "EDA", "responses": ["EDA which refers to Exploratory Data Analysis is a process to understand the data prior getting it into machine learning pipeline."]}, {"tag": "avoid Bias", "responses": ["Bias can cause to feel or show inclination or prejudice for or against someone or something. Avoiding bias in machine learning is very important, and the last thing we would want is to create a model which will most of the times/always classify a non-defective product as a defective one."]}, {"tag": "Z Score", "responses": ["The z-score is the standard distortion count from a data point on average. But technically this is a source of how many constant changes are above or above the population. A z-score is known as a fixed value and can be placed in a normal distribution ramp. It eliminates values from the database that are lower than Z times 3 times."]}, {"tag": "odds ratio", "responses": ["The odds ratio is the odds within two groups. For example, let\u2019s pretend that we are trying to determine the effectiveness of medicine. We administered the medication to the \u2018intervention\u2019 organization and a position to the \u2018control\u2019 group."]}, {"tag": "classification problems", "responses": ["Accuracy is not a good basis for distribution problems because it provides equal significant value to both false positives and false negatives dataset value. Accuracy provides equal quality to both cases and cannot distinguish between them."]}, {"tag": "trade-off between accuracy and interpretability", "responses": ["There needs to be a trade-off between accuracy and interpretability. Neural networks spit out the best possible result, and we can\u2019t ignore that just because we don\u2019t understand the internal functioning of the model."]}, {"tag": "missing data", "responses": ["No. Not only the blanks, data points which has NA, NULL and also sometimes the corrupted data that has been recorded by mistake or given improper data by purpose."]}, {"tag": "gradient descent methods converge", "responses": ["No, they do not because in some cases it reaches an local minima or a local optima points. You don\u2019t reach to the global optima point. It depends on the data and starting the conditions."]}, {"tag": "PATTERN RECOGNITION", "responses": ["Pattern Recognition can be used in Computer Vision, Speech Recognition, Data Mining, Statistics, Informal Retrieval, Bio-Informatics"]}, {"tag": "genetic programming", "responses": ["Genetic programming is one of the two techniques used in machine learning. The model is based on the testing and selecting the best choice among a set of results."]}, {"tag": " Inductive Logic Programming", "responses": ["  Inductive Logic Programming (ILP) is a subfield of machine learning which uses logical programming representing background knowledge and examples."]}, {"tag": "model selection", "responses": [" The process of selecting models among different mathematical models, which are used to describe the same data set is known as Model Selection."]}, {"tag": "calibration in Supervised Learning", "responses": [" The two methods used for predicting good probabilities in Supervised Learning are Platt Calibration, Isotonic Regression"]}, {"tag": "overfitting", "responses": [" When there is sufficient data \u2018Isotonic Regression\u2019 is used to prevent an overfitting issue."]}, {"tag": " difference between heuristic for rule learning and heuristics", "responses": ["The difference is that the heuristics for decision trees evaluate the average quality of a number of disjointed sets while rule learners only evaluate the quality of the set of instances that is covered with the candidate rule."]}, {"tag": "  Perceptron", "responses": ["In Machine Learning, Perceptron is an algorithm for supervised classification of the input into one of several possible non - binary output "]}, {"tag": " components of Bayesian logic program", "responses": ["Bayesian logic program consists of two components. The first component is a logical one ; it consists of a set of Bayesian Clauses, which captures the qualitative structure of the domain.The second component is a quantitative one, it encodes the quantitative information about the domain."]}, {"tag": "bayesian networks", "responses": ["Bayesian Network is used to represent the graphical model for probability relationship among a set of variables."]}, {"tag": "Lazy learning algorithm", "responses": ["Instance based learning algorithm is also referred as Lazy learning algorithm as they delay thE induction or generalization process until classification is performed."]}, {"tag": "Support Vector Machine", "responses": ["Combining binary classifiers, Modifying binary to incorporate multiclass learning."]}, {"tag": "cross-validation?", "responses": ["Cross-validation is essentially a technique used to assess how well a model performs on a new independent dataset.The simplest example of cross - validation is when you split your data into two groups: training data and testing data, where you use the training data to build the model and the testing data to test the model."]}, {"tag": "ensemble learning?", "responses": ["Ensemble learning is used when you build component classifiers that are more accurate and independent from each other."]}, {"tag": "  basic data structures", "responses": [" Data structures - Array, Linked List, Binary Tree, Balanced Tree Libraries\u2013 NumPy, SciPy, Scikit Learn, Theano, TensorFlow, Keras, PyTorch, Pandas, Matplotlib "]}, {"tag": " Types of machine learning", "responses": ["Various types of machine learning are -Supervised Learning (Task driven), Unsupervised Learning (Data Driven),  Reinforcement Learning (Learning from errors)"]}, {"tag": "Hypothesis Generation", "responses": ["Hypothesis generation is a process of creating a set of features which could influence the target variable given a confidence interval (taken as 95% all the time). We can do this before looking at the dataset to avoid biased thoughts. This step often helps in creating new features."]}, {"tag": "Data Wrangling", "responses": [" Data Wrangling is the first step we take while creating a Machine Learning model. This is the main step in which we prepare data for a Machine Learning algorithm.This step is very crucial and takes up to 60 to 80 percent of time."]}, {"tag": "labeled and unlabeled data", "responses": ["Typically, unlabeled data consists of samples of natural or human-created artifacts that we can obtain relatively easily from the world. Some examples of unlabeled data might include photos, audio recordings, videos, news articles etc.Labeled data typically takes a set of unlabeled data and augments each piece of that unlabeled data with some sort of meaningful tag,label, or class that is somehow informative or desirable to know. For example, labels for the above types of unlabeled data might be whether this photo contains a horse or a cow, which words were uttered in this audio recording, what type of action is being performed in this video, what the topic of this news article is etc."]}, {"tag": "features and labels", "responses": ["  Label: Labels are referred to as the final output. The output classes can also be considered as labels.When data scientists speak of labeled data what they mean is a group of samples which have been tagged to one or more labels. Feature: Features are defined as individual independent variables that act as input. Prediction models use features to make predictions.Moreover, you can also obtain new features from old features using a feature engineering method. Example - suppose we want to categorize our friends by their weight and height into different groups like obese, fit, underweight.Here, the inputs are weight and height which acts as a feature and the group names(fit, obese, underweight) which are the final outcome is the label."]}, {"tag": "Noise", "responses": [" Noise is unwanted data items, features or records which don\u2019t help in explaining the feature itself, or the relationship between feature & target.Noise often causes the algorithms to miss out patterns in the data "]}, {"tag": " imbalanced datasheet", "responses": ["  Imbalanced dataset is relevant primarily in the context of supervised machine learning involving two or more classes. Imbalance means that the number of data points available for different the classes is different: If there are two classes, then balanced data would mean 50 % points for each of the class.For most machine learning techniques, little imbalance is not a problem.So, if there are 60 % points for one class and 40 % for the other class, it should not cause any significant performance degradation.Only when the class imbalance is high, e.g .90 % points for one class and 10 % for the other, standard optimization criteria or performance measures may not be as effective and would need modification."]}, {"tag": "Linear Discriminant Analysis", "responses": ["  Linear Discriminant Analysis or LDA is a dimensionality reduction technique. It is used as a pre - processing step in Machine Learning and applications of pattern classification.The goal of LDA is to project the features in higher dimensional space onto a lower - dimensional space in order to avoid the curse of dimensionality and also reduce resources and dimensional costs."]}, {"tag": " Gradient Descent", "responses": ["  Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model."]}, {"tag": " Regularization", "responses": [" The word regularize means to make things regular or acceptable. This is exactly why we use it for.Regularizations are techniques used to reduce the error by fitting afunction appropriately on the given training set and avoid overfitting."]}, {"tag": "  Regularization in Machine Learning", "responses": [" At times when the model begins to underfit or overfit, regularization becomes necessary. It is a regression that diverts or regularizes the coefficient estimates towards zero.It reduces flexibility and discourages learning in a model to avoid the risk of overfitting.The model complexity is reduced and it becomes better at predicting."]}, {"tag": " Classification model", "responses": [" A classification model tries to draw some conclusion from the input values given for training.It will predict the class labels / categories for the new data."]}, {"tag": " issues to consider in supervised learning", "responses": ["There are four major issues to consider in supervised learning:Bias - variance tradeoff, Function complexity and amount of training data, Dimensionality of the input space, Noise in the output values "]}, {"tag": "PATTERN RECOGNITION", "responses": ["Pattern Recognition can be used in Computer Vision, Speech Recognition, Data Mining, Statistics, Informal Retrieval, Bio-Informatics"]}, {"tag": "genetic programming", "responses": ["Genetic programming is one of the two techniques used in machine learning. The model is basedon the testing and selecting the best choice among a set of results."]}, {"tag": " Inductive Logic Programming", "responses": ["  Inductive Logic Programming (ILP) is a subfield of machine learning which uses logicalprogramming representing background knowledge and examples."]}, {"tag": "model selection", "responses": [" The process of selecting models among different mathematical models, which are used to describe the same data set is known as Model Selection."]}, {"tag": "calibration in Supervised Learning", "responses": [" The two methods used for predicting good probabilities in Supervised Learning are Platt Calibration, Isotonic Regression"]}, {"tag": "overfitting", "responses": [" When there is sufficient data \u2018Isotonic Regression\u2019 is used to prevent an overfitting issue."]}, {"tag": " difference between heuristic for rule learning and heuristics", "responses": ["The difference is that the heuristics for decision trees evaluate the average quality of a number of disjointed sets while rule learners only evaluate the quality of the set of instancethat is covered with the candidate rule."]}, {"tag": "  Perceptron", "responses": ["In Machine Learning, Perceptron is an algorithm for supervised classification of the input into one of several possible non-binary output"]}, {"tag": " components of Bayesian logic program", "responses": ["Bayesian logic program consists of two components. The first component is a logical one ; itconsists of a set of Bayesian Clauses, which captures the qualitative structure of the domain. The second component is a quantitative one, it encodes the quantitative information about the domain."]}, {"tag": "bayesian networks", "responses": ["Bayesian Network is used to represent the graphical model for probability relationship among a set of variables."]}, {"tag": "Lazy learning algorithm", "responses": ["Instance based learning algorithm is also referred as Lazy learning algorithm as they delay the induction or generalization process until classification is performed."]}, {"tag": "Support Vector Machine", "responses": ["Combining binary classifiers, Modifying binary to incorporate multiclass learning."]}, {"tag": "cross-validation?", "responses": ["Cross-validation is essentially a technique used to assess how well a model performs on a newi ndependent dataset. The simplest example of cross-validation is when you split your data into two groups: training data and testing data, where you use the training data to build the model and  the testing data to test the model."]}, {"tag": "ensemble learning?", "responses": ["Ensemble learning is used when you build component classifiers that are more accurate and independent from each other."]}, {"tag": "  basic data structures", "responses": [" Data structures - Array, Linked List, Binary Tree, Balanced Tree Libraries \u2013 NumPy, SciPy, Scikit Learn, Theano, TensorFlow, Keras, PyTorch, Pandas, Matplotlib"]}, {"tag": " Types of machine learning", "responses": ["Various types of machine learning are -Supervised Learning (Task driven), Unsupervised Learning (Data Driven),  Reinforcement Learning (Learning from errors)"]}, {"tag": "Hypothesis Generation", "responses": ["Hypothesis generation is a process of creating a set of features which could influence the target variable given a confidence interval (taken as 95% all the time). We can do this before looking at the dataset to avoid biased thoughts. This step often helps in creating new features."]}, {"tag": "Data Wrangling", "responses": [" Data Wrangling is the first step we take while creating a Machine Learning model. This is the main step in which we prepare data for a Machine Learning algorithm. This step is very crucial and takes up to 60 to 80 percent of time."]}, {"tag": "labeled and unlabeled data", "responses": ["  Typically, unlabeled data consists of samples of natural or human-created artifacts that wecan obtain relatively easily from the world. Some examples of unlabeled data might include photos, audio recordings, videos, news articles etc. Labeled data typically takes a set of unlabeled data and augments each piece of that unlabeled data with some sort of meaningful 'tag', 'label', or 'class' that is somehow informative or desirable to know. For example, labels for the above types of unlabeled data might be whether this photo contains a horse or a cow, which words were uttered in this audio recording, what type of action is being performed in this video, what the topic of this news article is etc."]}, {"tag": "features and labels", "responses": ["  Label: Labels are referred to as the final output. The output classes can also be considered as labels. When data scientists speak of labeled data what they mean is a group of samples whichhave been tagged to one or more labels.Feature: Features are defined as individual independent variables that act as input. Prediction models use features to make predictions. Moreover, you can also obtain new features from old features using a feature engineering method. Example- suppose we want to categorize our friends by their weight and height into different groups like obese, fit, underweight. Here, the inputs are weight and height which acts as a feature and the group names (fit, obese, underweight) which are the final outcome is the label. "]}, {"tag": "Noise", "responses": [" Noise is unwanted data items, features or records which don\u2019t help in explaining the feature itself, or the relationship between feature & target. Noise often causes the algorithms to miss out patterns in the data  "]}, {"tag": " imbalanced datasheet", "responses": ["  Imbalanced dataset is relevant primarily in the context of supervised machine learninginvolving two or more classes.Imbalance means that the number of data points available for different the classes is different:If there are two classes, then balanced data would mean 50% points for each of the class. Formost machine learning techniques, little imbalance is not a problem. So, if there are 60% points for one class and 40% for the other class, it should not cause any significant performance degradation. Only when the class imbalance is high, e.g. 90% points for one class and 10% for the other, standard optimization criteria or performance measures may not be as effective and would need modification."]}, {"tag": "Linear Discriminant Analysis", "responses": ["  Linear Discriminant Analysis or LDA is a dimensionality reduction technique. It is used as a pre-processing step in Machine Learning and applications of pattern classification. The goal of LDA is to project the features in higher dimensional space onto a lower-dimensional space in order to avoid the curse of dimensionality and also reduce resources and dimensional costs. "]}, {"tag": " Gradient Descent", "responses": ["  Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model."]}, {"tag": " Regularization", "responses": [" The word regularize means to make things regular or acceptable. This is exactly why we useit for. Regularizations are techniques used to reduce the error by fitting a function appropriately on the given training set and avoid overfitting."]}, {"tag": "  Regularization in Machine Learning", "responses": [" At times when the model begins to underfit or overfit, regularization becomes necessary. It is a regression that diverts or regularizes the coefficient estimates towards zero. It reduces flexibility and discourages learning in a model to avoid the risk of overfitting. The model complexity is reduced and it becomes better at predicting. "]}, {"tag": " Classification model", "responses": [" A classification model tries to draw some conclusion from the input valuesgiven for training. It will predict the class labels/categories for the new data."]}, {"tag": " issues to consider in supervised learning", "responses": ["There are four major issues to consider in supervised learning: Bias-variance tradeoff, Function complexity and amount of training data, Dimensionality of the input space, Noise in the output values "]}, {"tag": "algorithm is used best for supervised learning", "responses": ["A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems."]}, {"tag": " standard supervised learning", "responses": ["There are several ways in which the standard supervised learning problem canbe generalized: Semi-supervised learning: In this setting, the desired output values are provided only for a subset of the training data.The remaining data is labeled. Weak supervision: In this setting, noisy, limited, or imprecise sources are used to provide supervision signal for labeling training data."]}, {"tag": "  applications of supervised learning", "responses": ["   Some applications of supervised learning are -Bioinformatics,spam detection,voice recognition, pattern recognition"]}, {"tag": "     Support Vector Machine", "responses": ["In this algorithm, we plot each data item as a point in n-dimensional space (where n is a number of features you have) with the value of each feature being the value of a particular coordinate. "]}, {"tag": "advantages of supervised learning", "responses": ["Advantages of supervised learning areSupervised learning allows you to collect data or produce a data output from the previous experience Helps you to optimize performance criteria using experience Supervised machine learning helps you to solve various types of real-world computation problems. "]}, {"tag": " disadvantages of supervised learning", "responses": ["Disadvantages of supervised learning areDecision boundary might be overtrained if your training set which doesn't have examples that you want to have in a class You need to select lots of good examples from each class while you are training the classifier."]}, {"tag": "   Decision Trees", "responses": ["Decision Trees (DT) are trees that classify instances by sorting them based on feature values, where each node in a decision tree represents a feature in an instance to be classified, and each branch represents a value that the node can assume. Instances are classified starting at the root node and sorted based on their feature values"]}, {"tag": "K-Nearest Neighbor Algorithm", "responses": ["The k-nearest neighbour kernel density estimation method is a special type of the kernel density estimation method with the local choice of thebandwidth. An advantage of this estimator is that smoothing varies according to the number of observations in a particular region. The crucial problem is how to estimate the value of the parameter k. K is a kernel function which satisfies \u222b K(x) dx = 1 "]}, {"tag": "neural networks algorithms", "responses": ["Neural nets take inspiration from the learning process occurring in human brains. They consists of an artificial network of functions, called parameters,which allows the computer to learn, and to fine tune itself, by analyzing new data. "]}, {"tag": "algorithms used by neural networks.", "responses": ["Gradient descent,  Newton\u2019s method, Conjugate gradient"]}, {"tag": "best practices for supervised learning", "responses": ["Before doing anything else, you need to decide what kind of data is to be used as a training set.You need to decide the structure of the learned function and learning algorithm. Gathere corresponding outputs either from human experts or from measurements"]}, {"tag": "self supervised learning", "responses": ["Self-supervised learning is a means for training computers to do tasks without humans providing labeled data (i.e., a picture of a dog accompanied by thelabel \u201cdog\u201d). It is a subset of unsupervised learning where outputs or goals are derived by machines that label, categorize, and analyze information on their own then draw conclusions based on connections and correlations"]}, {"tag": " applications of self-supervised learning", "responses": ["According to Yann Lecun, a computer scientist known for his impressive work in the ML field, the closest we have to self-supervised learning systems are the so-called \u201cTransformers.\u201d These are ML models that successfully use natural language processing (NLP) without the need for labeled datasets. They are capable of processing massive amounts of unstructured data and \u201ctransform\u201d them into usable information for various purposes. The Transformers are behind Google\u2019s BERT and Meena, OpenAI\u2019s GPT2, and Facebook\u2019s RoBERTa. "]}, {"tag": " Polynomial Regression", "responses": ["Polynomial Regression is a form of linear regression in which the relationshipbetween the independent variable x and dependent variable y is modeled as an nth degree polynomial. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y |x) "]}, {"tag": "using Polynomial Regression", "responses": ["There are some relationships that a researcher will hypothesize is curvilinear. Clearly, such type of cases will include a polynomial term. Inspection of residuals. If we try to fit a linear model to curved data, a scatter plot of residuals (Y axis) on the predictor (X axis) will have patches of many positive residuals in the middle. Hence in such situation it is not appropriate. An assumption in usual multiple linear regression analysis is that all the independent variables are independent. In polynomial regression model, this assumption is not satisfied."]}, {"tag": " advantages of using Polynomial Regression over Linear Regression", "responses": ["Broad range of function can be fit under it. Polynomial basically fits wide range of curvature. Polynomial provides the best approximation of the relationship between dependent and independent variable."]}, {"tag": " Ridge Regression technique", "responses": [" Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity. When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors. It is hoped that the net effect will be to give estimates that are more reliable."]}, {"tag": "importance of unsupervised learning", "responses": ["Unsupervised learning is a machine learning technique, where we do not need to supervise the model. Instead, we need to allow the model to work on its own to discover information. It\u2019s important to gain insight into the way the system behaves without supervision. For example, GAN, CNN, K-NN, etc.Deep learning can be either supervised or unsupervised. It all depends on how we want to use it. Deep learning is used for highly scalable datasets and algorithms which are complex for machine learning algorithms like SVM, Logistic Regression etc., to model. For example, Convolutional Neural Network (CNN) can be both supervised or unsupervised. If we want to classify images, we need to add dense layers and for classification, the training will be supervised. But, if we want to cluster images based on similarities of a group of images, we will extract the features using CNN and then use an unsupervised method like k-means or DBSCAN clustering."]}, {"tag": "principle of Unsupervised Data Augmentation", "responses": ["  Unsupervised Data Augmentation or UDA is a semi-supervised learning method which achieves state-of-the-art results on a wide variety of language and vision tasks. UDA is a method of semi-supervised learning, that reduces the need for labeled examples and better utilizes unlabeled ones"]}, {"tag": "use of auto encoder", "responses": ["Both PCA and Autoencoders are dimensional reduction techniques but Autoencoders are used more for detecting anomalies than for reducing data. Using Autoencoder, we try to reconstruct our data and then use reconstruction error to identify anomalous data points. This reconstruction is something that PCA is not very good at, especially when you have non-linear relationships among variables. Autoencoders, on the other hand, can use deep architecture to learn complex non-linear relationships as well."]}, {"tag": "PCA or auto encoder", "responses": [" PCA will result in slower processing compared with an Autoencoder"]}, {"tag": "use of unsupervised clustering models", "responses": ["For supervised learning we need to have a labeled data set. If not, it is good to run unsupervised learning algorithms for automatically labeling unlabeled data. Once the data is labelled using clustering algorithms, then it is possible to use supervised learning algorithms. For linking the two tasks a simple script can be written that connect the output of clustering as an input for the classification task."]}, {"tag": "techniques for anomaly detection on a dataset", "responses": ["We can use the following clustering algorithm - 1.)K-means/medoid 2.)Fuzzy c-means 3.)partition-based clustering"]}, {"tag": "need of unsupervised learning", "responses": [" In unsupervised learning, we do not have a supervisor which tells us what is right and what is wrong i.e. we do not have input data with example output data where we want to learn the input/output relation. In general, we use unsupervised learning when we want to find general structures in our data. for example- major trends, a low-dimensional embedding or clustering."]}, {"tag": "unsupervised learning algorithm for categorical data", "responses": [" K-means algorithm can be used to generate labelled/categorical of data "]}, {"tag": "types of clustering methods", "responses": ["Different types of clustering methods are -   Density-based methods, Hierarchical based method, Partitioning methods, Grid-based methods"]}, {"tag": "Density based lagorithms", "responses": ["These methods consider the clusters as the dense region having some similarity and different from the lower dense region of the space. These methods have good accuracy and ability to merge two clusters. Example:  DBSCAN (Density-Based Spatial Clustering of Applications with Noise), OPTICS (Ordering Points to Identify Clustering Structure) etc."]}, {"tag": "hierarchical clustering based method", "responses": ["The clusters formed in this method forms a tree-type structure based on the hierarchy. New clusters are formed using the previously formed one. It is divided into two categories:1.)Agglomerative (bottom up approach) 2.)Divisive (top down approach) for example: CURE (Clustering Using Representatives), BIRCH (Balanced Iterative Reducing Clustering and using Hierarchies) etc."]}, {"tag": "partitioning clustering method", "responses": [" These methods partition the objects into k clusters and each partition forms one cluster. This method is used to optimize an objective criterion similarity function such as when the distance is a major parameter example K-means, CLARANS (Clustering Large Applications based upon Randomized Search) etc."]}, {"tag": "grid-based clsutering methods", "responses": ["In this method the data space is formulated into a finite number of cells that form a grid-like structure. All the clustering operation done on these grids are fast and independent of the number of data objects example STING (Statistical Information Grid), wave cluster, CLIQUE (Clustering in Quest) etc."]}, {"tag": "use of KNN algorithm", "responses": ["KNN can be used for both classification and regression predictive problems. However, it is more widely used in classification problems in the industry. To evaluate any technique, we generally look at 3 important aspects: 1. Ease to interpret output 2. Calculation time  3. Predictive Power. KNN algorithm fairs across all parameters of considerations. It is commonly used for its easy of interpretation and low calculation time."]}, {"tag": "use of KNN algorithm", "responses": ["KNN can be used for both classification and regression predictive problems. However, it is more widely used in classification problems in the industry. To evaluate any technique, we  enerally look at 3 important aspects: 1. Ease to interpret output  2. Calculation time  3. Predictive Power.  KNN algorithm fairs across all parameters of considerations. It is commonly used for its easy of interpretation and low calculation time."]}, {"tag": "working of knn algorithm", "responses": [" Let\u2019s take a simple case to understand this algorithm. Lets say there is a spread of red circles (RC) and green squares (GS) and you intend to find out the class of the blue star (BS). BS can either be RC or GS and nothing else. The \u201cK\u201d in KNN algorithm is the nearest neighbor we wish to take the vote from. Let\u2019s say K = 3. Hence, we will now make a circle with BS as the center just as big as to enclose only three datapoints on the plane. The three closest points to BS are all RC. Hence, with a good confidence level, we can say that the BS should belong to the class RC. Here, the choice became very obvious as all three votes from the closest neighbor went to RC. The choice of the parameter K is very crucial in this algorithm. Next, we will understand what are the factors to be considered to conclude the best K."]}, {"tag": "disadvantages of knn algorithm", "responses": [" Disadvantages of KNN algorithm are: The algorithm gets significantly slower as the number of examples and/or predictors/independent variables increase."]}, {"tag": "working of k-means algorithm", "responses": [" To process the learning data, the K-means algorithm in data mining starts with a first group of randomly selected centroids, which are used as the beginning points for every cluster, and then performs iterative (repetitive) calculations to optimize the positions of the centroids It halts creating and optimizing clusters when either: 1.)The centroids have stabilized \u2014 there is no change in their values because the clustering has been successful. 2.)The defined number of iterations has been achieved."]}, {"tag": "advantages of k-means algorithm", "responses": [" The advantages of K-means algorithm are -   1.)Relatively simple to implement.  2.)Scales to large data sets.  3.)Guarantees convergence. 4.)Can warm-start the positions of centroids.  5.)Easily adapts to new examples. 6.) Generalizes to clusters of different shapes and sizes, such as elliptical clusters."]}, {"tag": "disadvantges of using k-means", "responses": [" The disadvantages of K-means algorithm are: 1.) Clustering data of varying sizes and density. 2.) Choosing k manually. 3.) Clustering outliers: Centroids can be dragged by outliers, or outliers might get their own cluster instead of being ignored. Consider removing or clipping outliers before clustering. 4.)Scaling with number of dimensions: As the number of dimensions increases, a distancebased similarity measure converges to a constant value between any given examples. 4.)Reduce dimensionality either by using PCA on the feature data, or by using \u201cspectral clustering\u201d to modify the clustering algorithm."]}, {"tag": "advantages of knn algorithm", "responses": ["The advantages of KNN algorithm are \u2013  1.)The algorithm is simply and easy to implement, 2.)There\u2019s no need to build a model, tune several parameters, or make additional assumptions 3.)The algorithm is versatile. It can be used for classification, regression, and search."]}]}